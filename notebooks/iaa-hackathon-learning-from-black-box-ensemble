{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":108047,"databundleVersionId":13798984,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/negoto/iaa-hackathon-learning-from-black-box-ensemble?scriptVersionId=282986624\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Learning from Black-Box Ensemble\n\nBridging the gap between predictive performance and actuarial transparency using IML toolkits\n\n<img src=\"https://github.com/ryo-asashi/iml4actuaries/raw/main/images/iml4actuaries.jpg\" alt=\"iml4actuaries\" width=\"600\">","metadata":{}},{"cell_type":"markdown","source":"# 1. Summary\n\n- **Team/Participant Name:** Ryoichi Asashiba (FIAJ)  \n- **Final Model Score in Public Leaderboard (RMSLE):** 0.10991 (`submission_post.csv`)\n- **One-line highlight of your approach:**  \"Bridging the gap between predictive performance and actuarial transparency using IML toolkits\"\n\n**Brief Summary**:\n\nIn this notebook, I constructed a high-precision ensemble model using LightGBM, XGBoost, and CatBoost to predict housing prices.\nRecognizing that \"black-box\" models are often difficult to implement in actuarial pricing due to regulatory requirements, I applied Interpretable Machine Learning (IML) techniques, including a novel method called MID (Maximum Interpretation Decomposition) using the `midlearn` library (developed by the author).\nThis allowed me to extract a transparent, GLM/GAM-like structure from the complex ensemble, ensuring both high accuracy and clear explainability.\nGenerative AI was utilized throughout the process for coding efficiency, drafting clear narratives, and decorating the notebook.\n\n---","metadata":{"execution":{"iopub.status.busy":"2025-09-18T11:53:07.784126Z","iopub.execute_input":"2025-09-18T11:53:07.7844Z","iopub.status.idle":"2025-09-18T11:53:07.795818Z","shell.execute_reply.started":"2025-09-18T11:53:07.784378Z","shell.execute_reply":"2025-09-18T11:53:07.794483Z"}}},{"cell_type":"markdown","source":"# 2. Generative AI Integration\n\n**How I used Generative AI tools:**\n\n- I utilized *Gemini* and *ChatGPT*, not just as a text generator, but as a **\"Domain Consultant\"**, a **\"Coding/Writing Assistant\"**, and a **\"Painter\"**.\n\n**Specific use cases & Creative twists:**\n\n1. **Domain Knowledge Augmentation:**\n\n   When EDA revealed counter-intuitive pricing in the 'Edwards' neighborhood (large area but low price), I used the AI to understand the qualitative socio-economic background of the area. This validated my hypothesis that these data points were structural outliers (e.g., proximity to railroads) rather than random noise.\n\n   *\"I noticed a discrepancy in price-per-sqft in the 'Edwards' neighborhood. What can be the reason? Could we expect that other houses in Edwards also show low valuations regardless of size?\"*\n\n2. **Efficient Coding/Writing Assistance:**\n\n   I utilized LLMs to accelerate the coding process, particularly for syntax that I rarely use. For instance, I asked the AI to generate the specific Linux commands (`!apt-get`) required to install the R-backend for `midlearn` within the Python kernel. I also used it to draft the LaTeX formulae for the IML methods, ensuring mathematical precision without spending time on formatting.\n\n   *\"How can I install specific R packages (like `midr` and `RcppEigen`) in this Kaggle notebook environment?\"*, *\"Could you write the mathematical definition of 'Permutation Feature Importance' in LaTeX format?\"*\n\n3. **Semantic Data Processing:**\n\n   Instead of blindly imputing missing values, I prompted the AI to analyze `data_description.txt` to distinguish between \"True Missing\" and \"Meaningful Missing\" (e.g., where 'NA' means 'No Pool').\n\n   *\"Read the attached data description. Could you summarize the definition of 'NA' for each column and recommend whether I should impute them with the mode/mean or a specific value like `None` or 0?\"*\n\n4. **Visual & Aesthetic Enhancement:**\n\n   To make the notebook visually engaging and intuitively convey the core theme, I used image generation tools to create a custom header image. The visual metaphor represents the actuarial challenge of \"opening the black box\" of machine learning to reveal the structured logic inside.\n\n   *\"Could you create a header image illustrating the concept of 'opening a black box model' to reveal a clear geometric structure inside, using a style inspired by Greek mythology, Pandora's box?\"*\n\n---","metadata":{"execution":{"iopub.status.busy":"2025-09-18T11:53:07.784126Z","iopub.execute_input":"2025-09-18T11:53:07.7844Z","iopub.status.idle":"2025-09-18T11:53:07.795818Z","shell.execute_reply.started":"2025-09-18T11:53:07.784378Z","shell.execute_reply":"2025-09-18T11:53:07.794483Z"}}},{"cell_type":"markdown","source":"# 3. Step-by-Step Analysis","metadata":{"execution":{"iopub.status.busy":"2025-09-18T11:53:07.784126Z","iopub.execute_input":"2025-09-18T11:53:07.7844Z","iopub.status.idle":"2025-09-18T11:53:07.795818Z","shell.execute_reply.started":"2025-09-18T11:53:07.784378Z","shell.execute_reply":"2025-09-18T11:53:07.794483Z"}}},{"cell_type":"markdown","source":"## 3.1. Setup and Data Understanding\n\nIn this section, we set up the Kaggle environment by installing necessary libraries, including the R backend for `midlearn`, and load the datasets.","metadata":{}},{"cell_type":"code","source":"%%capture\n# Install Python libraries\n!pip install midlearn\n!pip install plotnine==0.15.1\n!pip install scikit-misc\n\n# Install R packages in Python notebook (required for midlearn backend)\n!apt-get update && apt-get install -y r-cran-rcppeigen r-cran-midr r-cran-khroma","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:52:10.525572Z","iopub.execute_input":"2025-12-01T00:52:10.525996Z","iopub.status.idle":"2025-12-01T00:53:04.330458Z","shell.execute_reply.started":"2025-12-01T00:52:10.525969Z","shell.execute_reply":"2025-12-01T00:53:04.329132Z"},"_kg_hide-input":false,"_kg_hide-output":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Utils\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# Data Processing\nimport numpy as np\nimport pandas as pd\n\n# Validation\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Optimization\nimport optuna\n\n# Predictive Models\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.linear_model import LinearRegression\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nfrom plotnine import *\nfrom midlearn import scale_color_theme, scale_fill_theme, color_theme\n\n# Model Interpretation\nfrom sklearn.inspection import partial_dependence, permutation_importance\nfrom midlearn import MIDExplainer\nimport shap","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:04.332777Z","iopub.execute_input":"2025-12-01T00:53:04.333101Z","iopub.status.idle":"2025-12-01T00:53:23.405616Z","shell.execute_reply.started":"2025-12-01T00:53:04.333049Z","shell.execute_reply":"2025-12-01T00:53:23.404542Z"},"_kg_hide-input":false,"_kg_hide-output":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Global configurations\nPATH   = Path(\"/kaggle/input/ai-assisted-hackathon-1\")\nTARGET = 'SalePrice'\n\n# Set ntrials to >0 to enable optimization\nOPTUNA_NTRIALS = {'lgb': 0, 'xgb': 0, 'cat': 0, 'voting_weights': 0}\n\n# Set plotnine theme and color palette\ntheme_set(theme_bw())\npalette = color_theme(\"Zissou 1\").palette(5)\n\n# Load data frames\ntrain = pd.read_csv(PATH / \"train.csv\")\ntest  = pd.read_csv(PATH / \"test.csv\")\nsub   = pd.read_csv(PATH / \"sample_linear_regression_predictions.csv\")\n\nprint(\"Data Frame Info:\",\n      f\"\\n train ... shape {train.shape} with {train.isna().sum().sum()} missing values\",\n      f\"\\n test .... shape {test.shape} with {test.isna().sum().sum()} missing values\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:23.406724Z","iopub.execute_input":"2025-12-01T00:53:23.407414Z","iopub.status.idle":"2025-12-01T00:53:23.512159Z","shell.execute_reply.started":"2025-12-01T00:53:23.407387Z","shell.execute_reply":"2025-12-01T00:53:23.511157Z"},"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2. Data Cleaning & Preprocessing\n\nWe begin by addressing missing values and standardizing data types across both the training and testing datasets to ensure data consistency for downstream modeling.","metadata":{}},{"cell_type":"markdown","source":"### 3.2.1. Missing Value Imputation\n\n**Leveraging AI for Logical Imputation**:\n\nOut of 80 features, 27 contain missing values.\nInstead of applying a generic mean or mode imputation, I prompted the AI to analyze `data_description.txt`.\nThe AI successfully categorized the missing values into \"**True Missing**\" vs. \"**Meaningful Missing**\" (e.g., NA in `PoolQC` means \"No Pool\", not \"Unknown\").\n\n**Imputation Rules Derived**:\n1. **Meaningful Missing** (Categorical): Filled with `\"None\"` for columns like `PoolQC`, `GarageType`, `BsmtQual`, where NA indicates the absence of a facility.\n2. **Meaningful Missing** (Numerical): Filled with `0` for corresponding dimensions like `GarageArea` or `TotalBsmtSF`.\n3. **Contextual Imputation**: For `LotFrontage`, I used the median value grouped by `Neighborhood`, as AI suggested that street frontage is highly dependent on the local zoning layout.\n4. **True Missing**: For `Electrical`, I used the mode in the training dataset (\"SBrkr\"), as there is only one record in the testing dataset that lacks the value.","metadata":{}},{"cell_type":"code","source":"# Count missing values\nmissing_df = pd.DataFrame({\n    \"train\": train.drop(TARGET, axis=1).isna().sum(),\n    \"test\" : test.isna().sum(),\n    \"dtype\": train.drop(TARGET, axis=1).dtypes\n})\n\nmissing_df = missing_df[missing_df[\"train\"] + missing_df[\"test\"] > 0].reset_index().melt(\n    id_vars=\"index\",\n    value_vars=[\"train\", \"test\"],\n    var_name=\"dataset\",\n    value_name=\"count\"\n)\n\nggplot(missing_df) + \\\n  geom_col(aes(\"index\", \"count+1\", fill=\"dataset\"), position=\"dodge\") + \\\n  scale_y_log10(breaks=[1, 2, 11, 101, 1001], labels=[0, 1, 10, 100, 1000]) + \\\n  scale_fill_manual(values=palette) + \\\n  labs(subtitle=\"Number of Missing Values\", x=\"\", y=\"\") + \\\n  theme(\n      axis_text_x=element_text(rotation=90, hjust=1),\n      figure_size=[8, 4]\n  )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:23.514145Z","iopub.execute_input":"2025-12-01T00:53:23.514427Z","iopub.status.idle":"2025-12-01T00:53:24.183256Z","shell.execute_reply.started":"2025-12-01T00:53:23.514406Z","shell.execute_reply":"2025-12-01T00:53:24.182061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get medians of LotFrontage for each Neighborhood\nlot_frontage_median  = train[\"LotFrontage\"].median()\nlot_frontage_medians_by_neighborhood = train \\\n  .groupby(\"Neighborhood\", observed=True)[\"LotFrontage\"] \\\n  .median()\n\n# Get mode of Electrical\nelectrical_mode = train[\"Electrical\"].mode().iloc[0]\n\n# Define Functions for Preprocessing Data Frames\ndef prep_fillna(data: pd.DataFrame) -> pd.DataFrame:\n    df = data.copy()\n\n    # 1. Meaningful Missing (Categorical) -> \"None\"\n    cols_fill_none = [\n        'Alley',\n        'MasVnrType',\n        'BsmtQual',\n        'BsmtCond',\n        'BsmtExposure',\n        'BsmtFinType1',\n        'BsmtFinType2',\n        'FireplaceQu',\n        'GarageType',\n        'GarageFinish',\n        'GarageQual',\n        'GarageCond',\n        'PoolQC',\n        'Fence',\n        'MiscFeature'\n    ]\n    df[cols_fill_none] = df[cols_fill_none].fillna(\"None\")\n\n    # 2. Meaningful Missing (Numerical) -> 0\n    cols_fill_zero = [\n        'MasVnrArea',\n        'BsmtFinSF1',\n        'BsmtFinSF2',\n        'BsmtUnfSF',\n        'TotalBsmtSF',\n        'BsmtFullBath',\n        'BsmtHalfBath',\n        'GarageYrBlt',\n        'GarageCars',\n        'GarageArea'\n    ]\n    df[cols_fill_zero] = df[cols_fill_zero].fillna(0)\n\n    # 3. Contextual Imputation (LotFrontage) -> Median in each Neighborhood\n    df[\"Electrical\"] = df[\"Electrical\"].fillna(electrical_mode)\n\n    # 4. True Missing (Electrical) -> Mode\n    df[\"LotFrontage\"] = (\n        df[\"LotFrontage\"]\n        .fillna(df[\"Neighborhood\"].map(lot_frontage_medians_by_neighborhood))\n        .fillna(lot_frontage_median)        \n    )\n    \n    return df\n\n# Apply preprocessing\ntrain = prep_fillna(train)\ntest  = prep_fillna(test)\n\nprint(\"Data Frame Info:\",\n      f\"\\n train ... shape {train.shape} with {train.isna().sum().sum()} missing values\",\n      f\"\\n test .... shape {test.shape} with {test.isna().sum().sum()} missing values\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:24.1844Z","iopub.execute_input":"2025-12-01T00:53:24.185126Z","iopub.status.idle":"2025-12-01T00:53:24.231077Z","shell.execute_reply.started":"2025-12-01T00:53:24.185088Z","shell.execute_reply":"2025-12-01T00:53:24.230116Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.2.2 Data Type Conversion\n\nWe standardize feature data types to ensure optimal performance. Specifically, integers are cast to `float` for numerical consistency, and object (string) features are converted to the `category` type to enable efficient native handling by Gradient Boosting models.","metadata":{}},{"cell_type":"code","source":"def prep_convert_dtypes(data: pd.DataFrame) -> pd.DataFrame:\n    df = data.copy()\n    \n    int_cols = df.select_dtypes(include='int').columns\n    df[int_cols] = df[int_cols].astype('float')\n    \n    int_cols = df.select_dtypes(include='object').columns\n    df[int_cols] = df[int_cols].astype('category')\n    \n    return df\n\n# Apply preprocessing\ntrain = prep_convert_dtypes(train)\ntest  = prep_convert_dtypes(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:24.232147Z","iopub.execute_input":"2025-12-01T00:53:24.23243Z","iopub.status.idle":"2025-12-01T00:53:24.308312Z","shell.execute_reply.started":"2025-12-01T00:53:24.232408Z","shell.execute_reply":"2025-12-01T00:53:24.307022Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.2.3. Feature Engineering\n\nTo further enhance the model's predictive power, we introduce Domain-Driven Feature Engineering. While Gradient Boosting models are capable of learning complex interactions, explicitly creating aggregate features based on real estate valuation principles can significantly reduce the required tree depth and improve generalization.\n\nWe constructed the following key features:\n\n- Total Living Space (TotalSF): The single most important driver of housing value. We aggregate TotalBsmtSF, 1stFlrSF, and 2ndFlrSF to capture the property's overall capacity.\n\n- Functional Utility (TotalBath): We created a consolidated metric for bathrooms by weighting \"Half Baths\" as 0.5, aligning with standard real estate listing practices.\n\n- Depreciation Factors (HouseAge, RemodelAge): Converting calendar years (e.g., \"Built in 1990\") into duration (e.g., \"20 years old at sale\") helps the model capture the linear effect of asset depreciation more effectively.","metadata":{}},{"cell_type":"code","source":"def prep_feature_engineering(data: pd.DataFrame) -> pd.DataFrame:\n    df = data.copy()\n    \n    # 1. Total Square Footage (Most Important)\n    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n    \n    # 2. Total Bathrooms\n    df['TotalBath'] = (\n        df['FullBath'] + \n        (0.5 * df['HalfBath']) + \n        df['BsmtFullBath'] + \n        (0.5 * df['BsmtHalfBath'])\n    )\n    \n    # 3. Total Porch Area\n    df['TotalPorchSF'] = (\n        df['OpenPorchSF'] + \n        df['EnclosedPorch'] + \n        df['3SsnPorch'] + \n        df['ScreenPorch'] + \n        df['WoodDeckSF']\n    )\n    \n    # 4. Age of House (at the time of sale)\n    df['HouseAge'] = (df['YrSold'] - df['YearBuilt']).clip(lower=0)\n    \n    # 5. Years Since Remodel\n    df['RemodelAge'] = (df['YrSold'] - df['YearRemodAdd']).clip(lower=0)\n    \n    return df\n\n# Apply Feature Engineering\ntrain = prep_feature_engineering(train)\ntest  = prep_feature_engineering(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:24.309566Z","iopub.execute_input":"2025-12-01T00:53:24.310015Z","iopub.status.idle":"2025-12-01T00:53:24.334212Z","shell.execute_reply.started":"2025-12-01T00:53:24.30999Z","shell.execute_reply":"2025-12-01T00:53:24.332879Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3. Exploratory Data Analysis\n\nPrior to modeling, we perform EDA to understand the data structure, focusing on the target distribution, key correlations, and potential outliers.\nWe utilize `plotnine`, a Python implementation of the \"Grammar of Graphics,\" to employ a layered visualization strategy that resonates with the actuarial community familiar with R's `ggplot2`.","metadata":{}},{"cell_type":"markdown","source":"### 3.3.1. Distribution of Target Variable\n\n**Key Observations**:\n\n- The target variable `SalePrice` exhibits a right-skewed distribution.\n- Applying a log-transformation (`np.log1p`) effectively approximates a normal distribution. This justifies the use of **RMSLE** (Root Mean Squared Logarithmic Error) as the evaluation metric, as it effectively approximates relative error, preventing high-value properties from disproportionately dominating the model training.","metadata":{}},{"cell_type":"code","source":"p1 = ggplot(train, aes(x=\"SalePrice\")) + \\\n  geom_histogram(fill=palette[0], bins=30) + \\\n  labs(title=\"Distribution of SalePrice\", subtitle=\"Original Scale\")\n\np2 = ggplot(train, aes(x=\"np.log1p(SalePrice)\")) + \\\n  geom_histogram(fill=palette[1], bins=30) + \\\n  labs(subtitle=\"Logarithmic Scale\", x=\"log(SalePrice+1)\")\n\ndisplay((p1 | p2) + theme(figure_size=[8, 4]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:24.335608Z","iopub.execute_input":"2025-12-01T00:53:24.335983Z","iopub.status.idle":"2025-12-01T00:53:24.876163Z","shell.execute_reply.started":"2025-12-01T00:53:24.335954Z","shell.execute_reply":"2025-12-01T00:53:24.875003Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.3.2. Correlation Matrix\n\n**Key Observations**\n\n- We identified 10 features with a strong positive correlation ($> 0.5$) to the log-transformed `SalePrice`, suggesting they are key drivers for the model.\n- We also detected high correlations ($> 0.8$) between certain feature pairs (e.g., `GarageCars` and `GarageArea`). While tree-based models are generally robust to multicollinearity, this redundancy suggests potential for feature selection or dimensionality reduction.","metadata":{}},{"cell_type":"code","source":"k = 16\ndf = train.copy()\ndf[\"log(SalePrice+1)\"] = np.log(df[\"SalePrice\"] + 1)\ndf = df.drop([\"Id\", \"SalePrice\"], axis=1)\ncorr = df.select_dtypes(include=[np.number]).corr()\ntop_cols = corr.abs().nlargest(k, \"log(SalePrice+1)\")[\"log(SalePrice+1)\"].index.tolist()\n\ncorr_df_top = (\n    df[top_cols]\n    .corr()\n    .reset_index()\n    .melt(id_vars='index', var_name='variable', value_name='correlation')\n)\n\ncorr_df_top['index'] = pd.Categorical(\n    corr_df_top['index'],\n    categories=top_cols,\n    ordered=True\n)\n\ncorr_df_top['variable'] = pd.Categorical(\n    corr_df_top['variable'],\n    categories=top_cols[::-1],\n    ordered=True\n)\n\np = (\n    ggplot(corr_df_top, aes(x='index', y='variable', fill='correlation'))\n    + geom_tile(color='white')\n    + geom_text(aes(label='round(correlation, 2)'), size=7)\n    + scale_fill_theme(\"Zissou 1@seq\")\n    + theme(\n        axis_text_x=element_text(rotation=90, hjust=1),\n        figure_size=(7, 7),\n        legend_position=\"none\"\n    )\n    + labs(\n        x='', y='',\n        title=\"Correlation Heatmap\",\n        subtitle=f'Top {k - 1} Correlations with SalePrice'\n    )\n)\n\ndisplay(p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:24.877156Z","iopub.execute_input":"2025-12-01T00:53:24.877485Z","iopub.status.idle":"2025-12-01T00:53:26.594869Z","shell.execute_reply.started":"2025-12-01T00:53:24.877458Z","shell.execute_reply":"2025-12-01T00:53:26.593851Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.3.3. Scatterplots and Outliers\n\n**Key Observations**:\n\n- We can visually observe strong linear, or slightly non-linear relationships between each predictor variable and the logarithm of `SalePrice`.\n- We found one or two outliers in the scatterplots for `TotalSF`, `GrLivArea`, `TotalBsmtSF`, and `1stFlrSF`.\n\n**Leveraging AI for Domain Knowledge**:\n\nNotably, there are distinct outliers belonging to the \"Edwards\" `Neighborhood` that deviate from the linear trend, represented by colored points in the following scatterplots.\n\nThe number of outliers is not enough to convince that the outliers are from \"Edwards\", but we could support the hypothesis by getting domain knowledge from LLM.\n> **LLM**: \"Edwards is historically a mixed/working-class neighborhood in West Ames. In the dataset, it is notorious for containing 'outlier' propertiesâ€”specifically large homes that are priced low due to poor condition, location near railroad tracks, or being 'white elephants' (too large for the area's market value). Treat the Price-Area relationship in Edwards as non-linear or distinct from premium areas like NridgHt.\"","metadata":{}},{"cell_type":"code","source":"plots = []\n\nfor i, col in enumerate(top_cols[1:13]):\n    p = ggplot(train, aes(x=col, y=\"np.log1p(SalePrice)\")) + \\\n      geom_point(aes(color=\"Neighborhood == 'Edwards'\"), alpha=.5) + \\\n      geom_smooth(color=palette[i % 3], method=\"lowess\", se=False, linetype=\":\") + \\\n      labs(x=\"\", y=\"log(SalePrice+1)\", subtitle=col) + \\\n      scale_color_theme(\"R3\") + \\\n      theme(legend_position=\"none\")\n    plots.append(p)\n\np = (\n    (plots[ 0] | plots[ 1] | plots[ 2]) /\n    (plots[ 3] | plots[ 4] | plots[ 5]) /\n    (plots[ 6] | plots[ 7] | plots[ 8]) /\n    (plots[ 9] | plots[10] | plots[11])\n)\n\ndisplay(p + theme(figure_size=[8, 8]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:26.597918Z","iopub.execute_input":"2025-12-01T00:53:26.598232Z","iopub.status.idle":"2025-12-01T00:53:32.799519Z","shell.execute_reply.started":"2025-12-01T00:53:26.598209Z","shell.execute_reply":"2025-12-01T00:53:32.798537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.4. Black-Box Modeling\n\n**Model Architecture:**\n\nWe employ a Voting Regressor combining LightGBM, XGBoost, and CatBoost. These tree-based ensemble models are chosen for their ability to capture complex non-linearities and interactions that standard GLMs might miss.\n\n**Robust Validation Scheme:**\n\nTo ensure stability and prevent overfitting, we adopted a rigorous 5-Fold Cross-Validation strategy throughout the entire pipeline:\n\n- **Hyperparameter Tuning**: We used Optuna to optimize hyperparameters. Crucially, the objective function calculates the RMSE using 5-Fold CV for each trial. This prevents the parameters from overfitting to a specific holdout set.\n\n- **Final Scoring**: The final ensemble weights were also determined based on the Out-of-Fold (OOF) predictions from the 5-Fold CV.\n  \n**Strategic Outlier Removal (Risk Segmentation):**\n\nPrior to training the main ensemble, we explicitly removed the identified structural outliers in the \"Edwards\" neighborhood (specifically, large properties with low valuations). These excluded instances are not ignored; they are delegated to a specialized Linear Model in the Postprocessing stage (see Section 3.4.5).","metadata":{}},{"cell_type":"code","source":"# Process Data Frames for Model Fitting\ndef is_outlier(X):\n    return (X['Neighborhood'] == 'Edwards') & (X['GrLivArea'] > 4000)\n\ntrain_outliers = is_outlier(train)\ntrain_X = train.drop([\"Id\", TARGET], axis=1)[~train_outliers]\ntrain_y = np.log1p(train[TARGET])[~train_outliers]\n\ntest_outliers = is_outlier(test)\ntest_X = test.drop([\"Id\"], axis=1)\n\nobject_columns = train_X.select_dtypes(include=['object']).columns\ntrain_X[object_columns] = train_X[object_columns].astype('category')\ntest_X[object_columns]  = test_X[object_columns].astype('category')\n\n# Containers for Models and Out-Of-Fold Predictions\nmodels_all = []\noof_all = []\nrmse_all = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:32.800496Z","iopub.execute_input":"2025-12-01T00:53:32.801128Z","iopub.status.idle":"2025-12-01T00:53:32.81943Z","shell.execute_reply.started":"2025-12-01T00:53:32.8011Z","shell.execute_reply":"2025-12-01T00:53:32.818291Z"},"_kg_hide-input":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.4.1. LightGBM","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 300),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n        \"random_state\": 42,\n        \"n_jobs\": -1,\n        \"verbosity\": -1,\n    }\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof = np.zeros(train_X.shape[0])\n\n    for train_idx, valid_idx in kf.split(train_X):\n        X_train, X_valid = train_X.iloc[train_idx], train_X.iloc[valid_idx]\n        y_train, y_valid = train_y.iloc[train_idx], train_y.iloc[valid_idx]\n\n        model = LGBMRegressor(**params)\n        model.fit(X_train, y_train)\n        oof[valid_idx] = model.predict(X_valid)\n\n    return mean_squared_error(oof, train_y, squared=False)\n\nif OPTUNA_NTRIALS['lgb']:\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=OPTUNA_NTRIALS['lgb'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:32.820758Z","iopub.execute_input":"2025-12-01T00:53:32.821134Z","iopub.status.idle":"2025-12-01T00:53:32.839002Z","shell.execute_reply.started":"2025-12-01T00:53:32.8211Z","shell.execute_reply":"2025-12-01T00:53:32.837481Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    'n_estimators': 572,\n    'learning_rate': 0.046903771722442915,\n    'num_leaves': 33,\n    'max_depth': 3,\n    'min_child_samples': 12,\n    'subsample': 0.5054252747922688,\n    'colsample_bytree': 0.5153424876847629,\n    'reg_alpha': 0.0013304950643993156,\n    'reg_lambda': 1.0366280528896745e-05,\n    'random_state': 42,\n    'n_jobs': -1,\n    'verbosity': -1,\n    'importance_type': 'gain'\n}\n\nif OPTUNA_NTRIALS['lgb']:\n    params.update(study.best_trial.params)\nprint(\"Parameters:\\n\", params, \"\\n\")\n\nkf   = KFold(n_splits=5, shuffle=True, random_state=42)\noof = np.zeros(train_X.shape[0])\nmodels = list()\n\nfor train_idx, valid_idx in kf.split(train_X):\n    X_train, X_valid = train_X.iloc[train_idx], train_X.iloc[valid_idx]\n    y_train, y_valid = train_y.iloc[train_idx], train_y.iloc[valid_idx]\n\n    model = LGBMRegressor(**params)\n    model.fit(X_train, y_train)\n    oof[valid_idx] = model.predict(X_valid)\n    models.append(model)\n\nrmse = np.sqrt(np.mean((oof - train_y) ** 2))\nprint(f\"RMSE: {rmse:.6f}\")\n\noof_all.append(oof)\nmodels_all.extend(models)\nrmse_all.append(rmse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:32.840263Z","iopub.execute_input":"2025-12-01T00:53:32.840603Z","iopub.status.idle":"2025-12-01T00:53:34.782358Z","shell.execute_reply.started":"2025-12-01T00:53:32.840574Z","shell.execute_reply":"2025-12-01T00:53:34.781334Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.4.2. XGBoost","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-3, 10.0, log=True),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n        \"random_state\": 42,\n        \"n_jobs\": -1,\n        \"verbosity\": 0,\n        \"enable_categorical\": True,\n    }\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof = np.zeros(train_X.shape[0])\n\n    for train_idx, valid_idx in kf.split(train_X):\n        X_train, X_valid = train_X.iloc[train_idx], train_X.iloc[valid_idx]\n        y_train, y_valid = train_y.iloc[train_idx], train_y.iloc[valid_idx]\n\n        model = XGBRegressor(**params)\n        model.fit(X_train, y_train)\n        oof[valid_idx] = model.predict(X_valid)\n\n    return mean_squared_error(oof, train_y, squared=False)\n\nif OPTUNA_NTRIALS['xgb']:\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=OPTUNA_NTRIALS['xgb'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:34.783157Z","iopub.execute_input":"2025-12-01T00:53:34.783415Z","iopub.status.idle":"2025-12-01T00:53:34.793832Z","shell.execute_reply.started":"2025-12-01T00:53:34.783393Z","shell.execute_reply":"2025-12-01T00:53:34.793054Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    'enable_categorical': True,\n    'n_estimators': 858,\n    'learning_rate': 0.02054410551729427,\n    'max_depth': 4,\n    'min_child_weight': 9.169602388467816,\n    'subsample': 0.5208438616384486,\n    'colsample_bytree': 0.5466341131307267,\n    'reg_alpha': 2.5255272269931653e-08,\n    'reg_lambda': 2.349931550864719e-08\n} \n\nif OPTUNA_NTRIALS['xgb']:\n    params.update(study.best_trial.params)\nprint(\"Parameters:\\n\", params, \"\\n\")\n\nkf   = KFold(n_splits=5, shuffle=True, random_state=42)\noof = np.zeros(train_X.shape[0])\nmodels = list()\n\nfor train_idx, valid_idx in kf.split(train_X):\n    X_train, X_valid = train_X.iloc[train_idx], train_X.iloc[valid_idx]\n    y_train, y_valid = train_y.iloc[train_idx], train_y.iloc[valid_idx]\n\n    model = XGBRegressor(**params)\n    model.fit(X_train, y_train)\n    oof[valid_idx] = model.predict(X_valid)\n    models.append(model)\n\nrmse = np.sqrt(np.mean((oof - train_y) ** 2))\nprint(f\"RMSE: {rmse:.6f}\")\n\noof_all.append(oof)\nmodels_all.extend(models)\nrmse_all.append(rmse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:34.796247Z","iopub.execute_input":"2025-12-01T00:53:34.797583Z","iopub.status.idle":"2025-12-01T00:53:39.51746Z","shell.execute_reply.started":"2025-12-01T00:53:34.797553Z","shell.execute_reply":"2025-12-01T00:53:39.516558Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.4.3. CatBoost","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        \"iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n        \"depth\": trial.suggest_int(\"depth\", 3, 12),\n        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-8, 10.0, log=True),\n        \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 1.0),\n        \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n        \"random_strength\": trial.suggest_float(\"random_strength\", 0.0, 10.0),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n        \"random_seed\": 42,\n        \"task_type\": \"CPU\",\n        \"verbose\": 0\n    }\n\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    oof = np.zeros(train_X.shape[0])\n\n    for train_idx, valid_idx in kf.split(train_X):\n        X_train, X_valid = train_X.iloc[train_idx], train_X.iloc[valid_idx]\n        y_train, y_valid = train_y.iloc[train_idx], train_y.iloc[valid_idx]\n\n        model = CatBoostRegressor(**params)\n        cat_features = X_train.select_dtypes(include=['category', 'object']).columns.tolist()\n        model.fit(X_train, y_train, cat_features=cat_features)\n        oof[valid_idx] = model.predict(X_valid)\n\n    return mean_squared_error(oof, train_y, squared=False)\n\nif OPTUNA_NTRIALS['cat']:\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=OPTUNA_NTRIALS['cat'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:39.518356Z","iopub.execute_input":"2025-12-01T00:53:39.518607Z","iopub.status.idle":"2025-12-01T00:53:39.528411Z","shell.execute_reply.started":"2025-12-01T00:53:39.518589Z","shell.execute_reply":"2025-12-01T00:53:39.527353Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"params = {\n    'verbose': 0,\n    'iterations': 793,\n    'learning_rate': 0.04368544828907038,\n    'depth': 5,\n    'l2_leaf_reg': 5.669156983253315e-06,\n    'bagging_temperature': 0.1676625155901807,\n    'border_count': 168,\n    'random_strength': 0.011234091867572377,\n    'subsample': 0.8351770211747447,\n    'colsample_bylevel': 0.5245312500697552\n}\n\nif OPTUNA_NTRIALS['cat']:\n    params.update(study.best_trial.params)\nprint(\"Parameters:\\n\", params, \"\\n\")\n\nkf   = KFold(n_splits=5, shuffle=True, random_state=42)\noof = np.zeros(train_X.shape[0])\nmodels = list()\n\nfor train_idx, valid_idx in kf.split(train_X):\n    X_train, X_valid = train_X.iloc[train_idx], train_X.iloc[valid_idx]\n    y_train, y_valid = train_y.iloc[train_idx], train_y.iloc[valid_idx]\n\n    model = CatBoostRegressor(**params)\n    cat_features = X_train.select_dtypes(include=['category', 'object']).columns.tolist()\n    model.fit(X_train, y_train, cat_features=cat_features)\n    oof[valid_idx] = model.predict(X_valid)\n    models.append(model)\n\nrmse = np.sqrt(np.mean((oof - train_y) ** 2))\nprint(f\"RMSE: {rmse:.6f}\")\n\noof_all.append(oof)\nmodels_all.extend(models)\nrmse_all.append(rmse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:53:39.52956Z","iopub.execute_input":"2025-12-01T00:53:39.529883Z","iopub.status.idle":"2025-12-01T00:54:16.88159Z","shell.execute_reply.started":"2025-12-01T00:53:39.529851Z","shell.execute_reply":"2025-12-01T00:54:16.880566Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.4.4. Ensemble Model\n\nBy combining the three gradient boosting models via optimized weighted averaging, we achieved a significant improvement in global model stability.","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        \"lgb\": trial.suggest_float(\"lgb\", 0, 1),\n        \"xgb\": trial.suggest_float(\"xgb\", 0, 1),\n    }\n    params['cat'] = 1 - params['lgb'] - params['xgb']\n\n    oof = params['lgb'] * oof_all[0] + params['xgb'] * oof_all[1] + params['cat'] * oof_all[2]\n\n    return mean_squared_error(oof, train_y, squared=False)\n\nparams = {'lgb': 0.038314906899623774, 'xgb': 0.5102541883254639}\n\nif OPTUNA_NTRIALS['voting_weights']:\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=OPTUNA_NTRIALS['voting_weights'])\n    params.update(study.best_trial.params)\n\nparams['cat'] = 1 - params['lgb'] - params['xgb']\noof = params['lgb'] * oof_all[0] + params['xgb'] * oof_all[1] + params['cat'] * oof_all[2]\n\nrmse = np.sqrt(np.mean((oof - train_y) ** 2))\nprint(f\"RMSE: {rmse:.6f}\")\n\nrmse_all.append(rmse)\n\nggplot() + \\\n  geom_abline(slope=1, intercept=0) + \\\n  geom_point(aes(x=train_y, y=oof, color=\"Neighborhood=='Edwards'\"),\n             data=train[~train_outliers], alpha=.5) + \\\n  geom_smooth(aes(x=train_y, y=oof), method=\"loess\", color=palette[0], linetype=\":\") + \\\n  scale_color_theme(\"R3\") + \\\n  labs(\n      x=\"Actual Values of log(SalePrice+1)\",\n      y=\"Out-of-Fold Predictions for log(SalePrice+1)\",\n      title=\"Ensemble Model\",\n      subtitle=f\"= {params['lgb']:.3f} * LightGBM + {params['xgb']:.3f} * XGBoost + {params['cat']:.3f} * CatBoost\"\n  ) + \\\n  theme(figure_size=[5.5, 5], legend_position=\"bottom\")","metadata":{"trusted":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2025-12-01T00:54:16.883097Z","iopub.execute_input":"2025-12-01T00:54:16.883768Z","iopub.status.idle":"2025-12-01T00:54:17.544571Z","shell.execute_reply.started":"2025-12-01T00:54:16.88373Z","shell.execute_reply":"2025-12-01T00:54:17.543433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create an ensemble model of LGBMRegressors\nestimators = [(f\"model_{i}\", model) for i, model in enumerate(models_all)]\nweights = [\n    params['lgb'] if isinstance(model, LGBMRegressor) else params['xgb'] if isinstance(model, XGBRegressor) else params['cat']\n    for model in models_all\n]\n\nensemble = VotingRegressor(estimators=estimators, weights=weights)\nensemble.estimators_ = [model for label, model in ensemble.estimators] # Skip the fitting process\n\n# Predict for testing dataset\npred_y = ensemble.predict(test_X)\nsub[TARGET] = np.expm1(pred_y) - 1\ndisplay(sub.head())\n\nsub.to_csv(\"submission_ensemble.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:54:17.545483Z","iopub.execute_input":"2025-12-01T00:54:17.545793Z","iopub.status.idle":"2025-12-01T00:54:18.067587Z","shell.execute_reply.started":"2025-12-01T00:54:17.545767Z","shell.execute_reply":"2025-12-01T00:54:18.066316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Validation of Outlier Strategy:**\n\nWhen applied to the \"Edwards\" outliers (which were excluded from training), **the ensemble model predicts standard high market values**, resulting in significant overestimation.\nThis behavior is expected and confirms our hypothesis: these properties follow a distinct, lower-value pricing curve that fundamentally differs from the general market trend. This validates the necessity of the stratified post-processing step to correct these specific predictions.","metadata":{}},{"cell_type":"code","source":"p1 = ggplot(train, aes(x=\"GrLivArea\", y=\"np.log1p(SalePrice)\")) + \\\n  geom_point(aes(color=\"Neighborhood=='Edwards'\"), alpha=.5) + \\\n  scale_color_theme(\"R3\") + \\\n  labs(subtitle=\"train\") + \\\n  lims(y=[10.5, 14], x=[0, 6200]) + \\\n  theme(legend_position=\"bottom\")\n\np2 = ggplot(test, aes(x=\"GrLivArea\", y=np.log1p(sub[\"SalePrice\"]))) + \\\n  geom_point(aes(color=\"Neighborhood=='Edwards'\"), alpha=.5) + \\\n  scale_color_theme(\"R3\") + \\\n  labs(subtitle=\"test (Ensemble Model)\") + \\\n  lims(y=[10.5, 14], x=[0, 6200]) + \\\n  theme(legend_position=\"bottom\")\n\np1 | p2 + theme(figure_size=[8, 4])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:54:18.068687Z","iopub.execute_input":"2025-12-01T00:54:18.068974Z","iopub.status.idle":"2025-12-01T00:54:18.74736Z","shell.execute_reply.started":"2025-12-01T00:54:18.068951Z","shell.execute_reply":"2025-12-01T00:54:18.745867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.4.5. Stratified Modeling for Structual Outliers\n\nTo correct the systematic overestimation identified in the previous step, we adopted a stratified approach, i.e., we isolated the 'Edwards' large-property segment and modeled it using a simple Linear Regression.","metadata":{}},{"cell_type":"code","source":"# Linear Interpolation\nlm = LinearRegression()\nlm.fit(X=train.loc[train_outliers, [\"GrLivArea\"]], y=np.log1p(train.loc[train_outliers, \"SalePrice\"]))\nsub.loc[test_outliers, [\"SalePrice\"]] = np.expm1(lm.predict(X=test.loc[test_outliers, [\"GrLivArea\"]]))\n\ndisplay(sub.loc[test_outliers])\nsub.to_csv(\"submission_post.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:54:18.74883Z","iopub.execute_input":"2025-12-01T00:54:18.749328Z","iopub.status.idle":"2025-12-01T00:54:18.782369Z","shell.execute_reply.started":"2025-12-01T00:54:18.749296Z","shell.execute_reply":"2025-12-01T00:54:18.781316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"p3 = ggplot(test, aes(x=\"GrLivArea\", y=np.log1p(sub[\"SalePrice\"]))) + \\\n  geom_point(aes(color=\"Neighborhood=='Edwards'\"), alpha=.5) + \\\n  scale_color_theme(\"R3\") + \\\n  labs(subtitle=\"test (Postprocessed)\") + \\\n  lims(y=[10.5, 14], x=[0, 6200]) + \\\n  theme(legend_position=\"bottom\")\n\np1 | p3 + theme(figure_size=[8, 4])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:54:18.783255Z","iopub.execute_input":"2025-12-01T00:54:18.784537Z","iopub.status.idle":"2025-12-01T00:54:19.45591Z","shell.execute_reply.started":"2025-12-01T00:54:18.78451Z","shell.execute_reply":"2025-12-01T00:54:19.454928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.4 Validation Results\n\n**Performance Evaluation:**\n\nTo ensure a reliable assessment of predictive power, we evaluated each model using the Root Mean Squared Logarithmic Error (RMSLE) on the Out-of-Fold validation sets.\nAs shown below, the ensemble strategy yields a clear performance lift, demonstrating robust predictive accuracy on unseen data.","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({\n    \"Model\": [\"LGBMRegressor\",\n              \"XGBRegressor\",\n              \"CatBoostRegressor\",\n              \"VotingRegressor\"],\n    \"RMSLE on train oof\": rmse_all\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:54:19.457131Z","iopub.execute_input":"2025-12-01T00:54:19.457455Z","iopub.status.idle":"2025-12-01T00:54:19.46902Z","shell.execute_reply.started":"2025-12-01T00:54:19.457424Z","shell.execute_reply":"2025-12-01T00:54:19.468291Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.5. Model Interpretation\n\n**The Core Challenge**:\n\nWhile our ensemble model achieves a low RMSLE, it remains inherently opaque (a \"black box\").\nTo bridge the gap between predictive performance and actuarial transparency, we employ widely adopted Interpretable Machine Learning (IML) methods.\n\n**Notation:**\n\nLet $f(X)$ be the black-box model (e.g., the Voting Regressor), where $X$ represents the feature vector.\nFor a specific analysis, we denote the subset of features of interest as $X_S$ and its complement (i.e., all other features) as $X_C$.","metadata":{}},{"cell_type":"markdown","source":"### 3.5.1. Permutation Feature Importance (PFI)\n\n**Methodology**:\n\n[Permutation Feature Importance (PFI)](https://christophm.github.io/interpretable-ml-book/feature-importance.html) measures the global importance of a feature by calculating the increase in the model's prediction error after the feature's values are randomly shuffled. This shuffling breaks the relationship between the feature and the target, effectively rendering the feature \"noise.\"\n\nFor a feature $j$, we generate a permuted dataset $X^{perm}$ where the values of feature $j$ are shuffled. The importance $I_j$ is defined as the increase in the expected loss:\n$$I_j = E[ L(y, f(X_j^{perm}, X_{-j})) ] - E[ L(y, f(X)) ]$$\nwhere $L$ is the loss function (e.g., RMSE). A higher $I_j$ indicates that the feature is more critical for the model's predictive accuracy.\n\n**Key Findings**:\n\n- The predictive power of the ensenble model is largely supported by `TotalSF` (Total Living Space) and `OverallQual` (Overall Quality).\n- These are followed by `Neighborhood`, confirming that \"Location, Size, and Quality\" are the three pillars of property valuation.","metadata":{}},{"cell_type":"code","source":"# Compute PFI\npfi_res = permutation_importance(\n    ensemble,\n    train_X,\n    train_y,\n    n_repeats=1,\n    random_state=42\n)\n\npfi_df = pd.DataFrame({\n    'feature': train_X.columns,\n    'importance': pfi_res['importances_mean']\n}).sort_values(by='importance', ascending=False)\n\npfi_df['feature'] = pd.Categorical(\n    pfi_df['feature'], categories=pfi_df['feature'].tolist()[::-1], ordered=True\n)\n\nimportant_features = pfi_df['feature'].head(9)\n\nggplot(pfi_df.head(20)) + \\\n  geom_col(aes(x='feature', y='importance', fill='importance')) + \\\n  labs(y=\"\") + \\\n  coord_flip() + \\\n  scale_fill_theme(\"Zissou 1@seq\") + \\\n  labs(title=\"Feature Importance\", subtitle=\"Permutation, Root mean square loss\") + \\\n  theme(legend_position=\"none\", figure_size=[6, 4])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:54:19.470012Z","iopub.execute_input":"2025-12-01T00:54:19.470712Z","iopub.status.idle":"2025-12-01T00:54:58.087322Z","shell.execute_reply.started":"2025-12-01T00:54:19.470675Z","shell.execute_reply":"2025-12-01T00:54:58.086186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.5.2. Partial Dependence (PD)\n\nTo understand the functional relationship between specific features and housing prices, we employ [Partial Dependence Plots (PDP)](https://christophm.github.io/interpretable-ml-book/pdp.html).\nThis visualizes the marginal effect of a feature on the predicted outcome, analogous to examining the relativistic curve in a GLM.\n\n**Methodology:**\n\nThe PDP estimates the expected prediction for the feature of interest $X_S$ by marginalizing (averaging) over the distribution of all other features $X_C$.\nThe estimated function is defined as:\n$$\\hat{f}_{PDP}(x_S) = E_{X_C}[f(x_S, X_C)] \\approx \\frac{1}{n} \\sum_{i=1}^{n} f(x_S, x_C^{(i)})$$\n\n**Key Observations:**\n\n- As illustrated in the plots below, the relationship between features such as `TotalSF` and the log-transformed `SalePrice` exhibits **monotonic yet non-linear patterns** (e.g., saturation effects). This confirms that a standard linear GLM would likely fail to capture these dynamics without manual interventions, such as splines or binning.","metadata":{}},{"cell_type":"code","source":"# Compute PDs\npdps = []\nfor i, feature in enumerate(tqdm(important_features)):\n    pd_res = partial_dependence(ensemble, train_X.sample(100), feature, percentiles=(0, 1))\n    \n    pd_df = pd.DataFrame({feature: pd_res[\"values\"][0], \"Prediction\": pd_res[\"average\"][0]})\n    \n    pdp = ggplot(pd_df, aes(feature, \"Prediction\"))\n\n    if feature == \"Neighborhood\":\n        pdp += geom_point(color=\"#505050\", size=3)\n        pdp += theme(axis_text_x=element_text(rotation=90, hjust=1))        \n    else:\n        pdp += geom_line(color=palette[i % 3], size=1.5)\n\n    pdp = pdp + labs(x=\"\", subtitle=feature)\n    pdps.append(pdp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:54:58.088312Z","iopub.execute_input":"2025-12-01T00:54:58.088604Z","iopub.status.idle":"2025-12-01T00:56:23.720511Z","shell.execute_reply.started":"2025-12-01T00:54:58.08858Z","shell.execute_reply":"2025-12-01T00:56:23.719575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize PD amd ICE Plots\np = (\n    (pdps[0] | pdps[1]) /\n    (pdps[3] | pdps[4] | pdps[5]) /\n    (pdps[6] | pdps[7] | pdps[8])\n) & lims(y = [11.95, 12.50])\n\ndisplay(p + theme(figure_size=[8, 6]))\ndisplay(pdps[2] + theme(figure_size=[8, 4]) + lims(y = [11.95, 12.50]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:56:57.424648Z","iopub.execute_input":"2025-12-01T00:56:57.424979Z","iopub.status.idle":"2025-12-01T00:56:59.377628Z","shell.execute_reply.started":"2025-12-01T00:56:57.424956Z","shell.execute_reply":"2025-12-01T00:56:59.376577Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.5.3. SHapley Additive exPlanation (SHAP)\n\nWhile PFI and PDP provide global insights, actuaries often need to explain specific individual predictions (e.g., \"Why is this specific house priced so high?\"), which can be addressed with [SHapley Additive exPlanation (SHAP)](https://christophm.github.io/interpretable-ml-book/shap.html).\n\n**Methodology:**\n\nSHAP leverages game theory to attribute the prediction of an individual instance to its feature contributions. Its core property is additivity, guaranteeing that the prediction is fully distributed among the features:\n\n$$f(x) = \\phi_0 + \\sum_{j=1}^{M} \\phi_j(x)$$\n\nwhere $\\phi_0 = E[f(X)]$ is the baseline prediction, and $\\phi_j(x)$ is the Shapley value for feature $j$.\n\n**Implementation Note:**\n\nIt is important to note that the `shap.TreeExplainer`â€”which provides exact and efficient calculations for tree modelsâ€”does not natively support the `sklearn.ensemble.VotingRegressor` interface.\nWhile model-agnostic methods (like Kernel SHAP) exist, they are computationally expensive and only provide approximations. Therefore, we utilize the XGBoost component as a representative proxy for the ensemble to compute SHAP values.\nGiven the high correlation between the component models, the interpretation derived from XGBoost is effectively transferable to the overall ensemble behavior.","metadata":{}},{"cell_type":"code","source":"# Construct SHAP Explainer for Tree Models\nshap_explainer = shap.TreeExplainer(\n    ensemble.estimators_[5],\n    feature_perturbation=\"tree_path_dependent\"\n)\n\n# Compute SHAP Values\nshap_values = shap_explainer(test_X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:56:25.676823Z","iopub.execute_input":"2025-12-01T00:56:25.677166Z","iopub.status.idle":"2025-12-01T00:56:27.380916Z","shell.execute_reply.started":"2025-12-01T00:56:25.677131Z","shell.execute_reply":"2025-12-01T00:56:27.380116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=2, ncols=1)\n\nplt.sca(axes[0])\nshap.plots.waterfall(shap_values[0], show=False)\naxes[0].set_title(f\"Prediction Breakdown: House #0\", fontsize=12)\n\nplt.sca(axes[1])\nshap.plots.waterfall(shap_values[1], show=False)\naxes[1].set_title(f\"Prediction Breakdown: House #1\", fontsize=12)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:56:27.384917Z","iopub.execute_input":"2025-12-01T00:56:27.385203Z","iopub.status.idle":"2025-12-01T00:56:28.655758Z","shell.execute_reply.started":"2025-12-01T00:56:27.385182Z","shell.execute_reply":"2025-12-01T00:56:28.654463Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.6. Surrogate Modeling\n\n**The Actuarial Need:**\n\nSHAP provides excellent local explanations (*\"why this specific prediction was made\"*), actuaries often require a global structural understanding to validate the model against business logic and to implement it as a rating plan.\nFor this need, we utilized [Maximum Interpretation Decomposition (MID)](https://arxiv.org/abs/2506.08338).","metadata":{}},{"cell_type":"markdown","source":"### 3.6.1. Maximum Interpretation Decomposition (MID)\n\n**Methodology:**\n\nMID aims to \"distill\" the pattern recognition capability of the black-box model into a transparent, GLM/GAM-like structure.\nIt searches for a structured interpretation model $g(X)$ that mimics the black-box $f(X)$.\nThe interpretation model $g(X)$ is restricted to an additive structure (e.g., Main Effects + Two-way Interactions):\n\n$$g(X) = g_{\\emptyset} + \\sum_{j} g_j(x_j) + \\sum_{j \\neq k} g_{jk}(x_j, x_k)$$\n\nwhere each component function is modeled by linear basis expansion.\n\n**Optimization & Constraints:**\n\nThe decomposition is solved by minimizing the squared loss between the black-box prediction and the interpretation model prediction.\n\n$$\\min_{g} \\sum_{i=1}^{n} (f(x^{(i)}) - g(x^{(i)}))^2 + \\lambda \\cdot \\Omega(g)$$\n\nCrucially, this optimization is subject to identifiability constraints (specifically, centering constraints) to ensure that main effects and interactions are uniquely separated and do not overlap with the intercept.\n\nThis ensures that we obtain a transparent, table-lookup-ready structure while retaining as much of the black-box model's accuracy as possible.\nTo perform this decomposition, we utilize `midlearn` ( [GitHub](https://github.com/ryo-asashi/midlearn) | [Docs](https://ryo-asashi.github.io/midlearn/) ), a Python library developed by the author which wraps the R package `midr` ( [GitHub](https://github.com/ryo-asashi/midr/) | [Docs](https://ryo-asashi.github.io/midr/) ).\n\n**Key Advantages for Actuaries:**\n\n- **Inherent Interpretability:**\n  \n  The resulting model is essentially a Generalized Additive Model (GAM), or GAM with second-order interactions (GAÂ²M). It decomposes risk into \"Base Rate + Additive Terms,\" which can be directly converted into standard actuarial rating tables.\n- **Computational Efficiency:**\n  \n  Unlike model-agnostic permutation methods that require computationally expensive sampling for every instance, MID solves a single convex optimization problem (penalized least squares). Once fitted, the explanation is instantaneous.\n- **Interaction Discovery:**\n  \n  MID automatically detects and quantifies key interactions (e.g., `GrLivArea` Ã— `Neighborhood`), visualizing how risk factors modify each other.","metadata":{}},{"cell_type":"code","source":"# Define a set of terms explicitly\nmid_terms = train_X.columns.tolist() + [\"GrLivArea:Neighborhood\"]\n\n# Fit a MID Surrogate Explainer\nexplainer = MIDExplainer(\n    estimator=ensemble,\n    penalty=0.1,\n    params_main=25,\n    singular_ok=True,\n    terms=mid_terms,\n    verbosity=0\n)\n\nexplainer.fit(test_X, y=np.log1p(sub[\"SalePrice\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:56:28.656974Z","iopub.execute_input":"2025-12-01T00:56:28.657407Z","iopub.status.idle":"2025-12-01T00:56:31.648263Z","shell.execute_reply.started":"2025-12-01T00:56:28.65738Z","shell.execute_reply":"2025-12-01T00:56:31.647056Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Technical Note on Target Vector:**\n\nTypically, `MIDExplainer` generates predictions internally using the provided estimator.\nHowever, our final submission includes external post-processing (the linear correction for outliers) that is not contained within the `ensemble` object. \nTherefore, we explicitly pass the **final post-processed predictions** as `y` to the `fit` method.\nThis ensures that the surrogate model explains the full hybrid pipeline, not just the raw ensemble.","metadata":{}},{"cell_type":"markdown","source":"### 3.6.2. Surrogate Model Fidelity\n\nTo quantify the approximation accuracy of our surrogate model, we utilize the **Uninterpreted Variation Ratio (UVR)**, defined as the complement of the Fidelity Score ($R^2$):\n$$\\text{UVR} = 1 - R^2(f(X),\\ g(X))$$\nThis metric represents the proportion of the black-box model's prediction variance that the interpretable model $g(X)$ fails to explain. A UVR close to 0% indicates that the surrogate model $g(X)$ is a near-perfect functional clone of the complex ensemble $f(X)$.","metadata":{}},{"cell_type":"code","source":"print(f\"Uninterpreted Variation Ratio: {explainer.ratio:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:56:31.649433Z","iopub.execute_input":"2025-12-01T00:56:31.64991Z","iopub.status.idle":"2025-12-01T00:56:31.657725Z","shell.execute_reply.started":"2025-12-01T00:56:31.649877Z","shell.execute_reply":"2025-12-01T00:56:31.656802Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since we fitted the MID interpreter on the test set predictions (to strictly explain our final submission), we must verify that the extracted structure is generalizable and not overfitted to specific artifacts in the test data.\n\nTo confirm this, we perform a consistency check (back-testing) by applying the fitted MID model back to the training dataset and comparing the MID predictions against the actual training labels (SalePrice).\n\n**Result:**\n\nThe plot below demonstrates a strong alignment between the MID predictions and actual values in the training set. This confirms that the extracted GLM-like structure is robust and captures the true market drivers rather than noise.","metadata":{}},{"cell_type":"code","source":"pred_y_all = explainer.predict(train)\ntrain_y_all = np.log1p(train['SalePrice'])\nrmse = np.sqrt(np.mean((pred_y_all - train_y_all) ** 2))\n\nggplot() + \\\n  geom_abline(slope=1) + \\\n  geom_point(aes(x=train_y_all, y=pred_y_all)) + \\\n  geom_smooth(aes(x=train_y_all, y=pred_y_all), method=\"loess\", color=palette[0], linetype=\":\") + \\\n  labs(\n      title=\"Surrogate Model Fidelity\",\n      subtitle=f\"Actual (train) vs Predicted, rmse = {rmse:.6f}\",\n      x=\"log(SalePrice+1)\",\n      y=\"Predictions (MID Surrogate Model)\"\n  ) + \\\n  theme(figure_size=[5, 5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:56:31.658844Z","iopub.execute_input":"2025-12-01T00:56:31.659195Z","iopub.status.idle":"2025-12-01T00:56:32.520258Z","shell.execute_reply.started":"2025-12-01T00:56:31.659165Z","shell.execute_reply":"2025-12-01T00:56:32.51917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.6.3. MID Importance\n\n**Description:**\n\nThis metric quantifies the global contribution of each term (Main Effects and Interactions) to the overall prediction variance.\nUnlike PFI which shuffles data, this measures the (average) magnitude of the learned component functions directly.","metadata":{}},{"cell_type":"code","source":"imp = explainer.importance()\nimp.plot(theme = \"Zissou 1@seq\", max_nterms=20) + \\\n  labs(title=\"Term-Effect Importance\", subtitle=\"Mean Absolute Effect Size\") + \\\n  theme(figure_size = (6, 4), legend_position=\"none\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:57:42.179107Z","iopub.execute_input":"2025-12-01T00:57:42.179445Z","iopub.status.idle":"2025-12-01T00:57:42.580874Z","shell.execute_reply.started":"2025-12-01T00:57:42.179424Z","shell.execute_reply":"2025-12-01T00:57:42.579537Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.6.4. MID Effect\n\n**Description:**\n\nHere we visualize the actual shape of the learned component functions $g_j(x_j)$.\n\n- Main Effect plots represent the \"Relativity Curves\" (or Risk Relativities) familiar to actuaries. Notice how MID captures the non-linear saturation of GrLivArea naturally.\n- Interaction plot visualizes how the `Neighborhood` factor adjusts the slope of `GrLivArea`.","metadata":{}},{"cell_type":"code","source":"mids = []\nfor i, feature in enumerate(important_features):\n    if feature == \"Neighborhood\":\n        p = explainer.plot(term=feature)\n    else:\n        p = explainer.plot(term=feature, color=palette[i % 3], size=1.5)\n    p = p + \\\n      labs(subtitle=feature, y=\"Effect Size\")\n    mids.append(p)\n\np = (\n    (mids[0] | mids[1]) /\n    (mids[3] | mids[4] | mids[5]) /\n    (mids[6] | mids[7] | mids[8])\n) & lims(y = [-.3, .3])\n\ndisplay(p + theme(figure_size = (7, 6)))\ndisplay(mids[2] +\n        lims(y = [-.3, .3]) +\n        theme(\n            axis_text_x=element_text(rotation=90, hjust=1),\n            figure_size=[7, 3]\n        ))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:58:22.147043Z","iopub.execute_input":"2025-12-01T00:58:22.147455Z","iopub.status.idle":"2025-12-01T00:58:24.067984Z","shell.execute_reply.started":"2025-12-01T00:58:22.147431Z","shell.execute_reply":"2025-12-01T00:58:24.066834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if imp.terms(main_effects=False):\n    term = imp.terms(main_effects=False)[0]\n    p = explainer.plot(term) + \\\n      labs(subtitle=f\"{term}\") + \\\n      theme(legend_position=\"bottom\", legend_key_width=380, legend_key_height=10, figure_size=[7, 6])\n    display(p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:56:35.227006Z","iopub.execute_input":"2025-12-01T00:56:35.227383Z","iopub.status.idle":"2025-12-01T00:56:35.946227Z","shell.execute_reply.started":"2025-12-01T00:56:35.22735Z","shell.execute_reply":"2025-12-01T00:56:35.944919Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.6.5. MID Breakdown\n\n**Description:**\n\nThis visualization decomposes a specific prediction into its constituent parts (Intercept + Sum of Effects).","metadata":{}},{"cell_type":"code","source":"p1 = (\n    explainer.breakdown(data=test_X, row=0).plot(theme=\"shap\") +\n    labs(subtitle=\"Breakdown: House #0\") +\n    theme(legend_position=\"none\")\n)\np2 = (\n    explainer.breakdown(data=test_X, row=1).plot(theme=\"shap\") +\n    labs(subtitle=\"Breakdown: House #1\") +\n    theme(legend_position=\"none\")\n)\np1 | p2 + theme(figure_size=[8, 4])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T00:56:35.947449Z","iopub.execute_input":"2025-12-01T00:56:35.948144Z","iopub.status.idle":"2025-12-01T00:56:36.978182Z","shell.execute_reply.started":"2025-12-01T00:56:35.948115Z","shell.execute_reply":"2025-12-01T00:56:36.976889Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 4. Findings and Insights\n\nThrough our analysis using the Voting Regressor and the midlearn toolkit, we derived the following three key conclusions:\n\n**High Accuracy via Ensemble Learning:**\n\n- The Voting Regressor (combining LightGBM, XGBoost, and CatBoost) successfully achieved high predictive performance. By aggregating diverse gradient boosting models, we minimized variance and attained a competitive RMSLE, establishing a robust \"Black-Box\" baseline.\n\n**Transparency through Standard IML:**\n\n- Standard IML techniques (PFI and PDP) allowed us to peek inside this black box. We confirmed that:\n  - GrLivArea and OverallQual are the dominant risk factors.\n  - The relationship between these features and price is strictly non-linear (e.g., saturation effects), confirming that traditional linear GLMs would underfit without manual spline adjustments.\n\n**Structural Simplicity Revealed by MID (The Core Insight):**\n\n- The application of Maximum Interpretation Decomposition (MID) provided the most significant insight:\n  - The MID surrogate model achieved an Uninterpreted Variation Ratio (UVR) of approximately 0.5%. This implies that 99.5% of the complex Ensemble's prediction logic was successfully distilled into a transparent structure.\n  - Surprisingly, this high fidelity was achieved by a GAÂ²M structure (Generalized Additive Model with Interactions) containing only one interaction term (GrLivArea:Neighborhood).\n  - This demonstrates that the vast complexity of the Ensemble model was essentially approximating a relatively simple structure: \"Base Effects + One Critical Interaction.\" For actuaries, this is a powerful findingâ€”it suggests we can achieve \"Black-Box accuracy\" with \"Glass-Box transparency\" without over-engineering the rating plan.\n  \n  *Note:* While the surrogate model shows high fidelity, we must remain vigilant about potential overfitting of the interpreter itself. As discussed in Section 3.6.2, structural stability checks (back-testing) are essential before deployment.\n\n**Note on Data Augmentation & Complience:**\n\n- We identified that augmenting the dataset with the original Ames Housing Data would effectively improve accuracy. We have verified in a separate experiment that introducing external data successfully improved the predictive power. (See our supplementary notebook: [IAA Hackathon: Experiment on Original Dataset](https://www.kaggle.com/code/negoto/iaa-hackathon-experiment-on-original-dataset))\n- However, for this submission, we **strictly adhered to using only the provided dataset** to ensure compliance with the competition's implicit constraints on data leakage.\n\n---","metadata":{}},{"cell_type":"markdown","source":"# 5. Reproducibility Note\n\nWe certify that this notebook is self-contained and fully reproducible. All code, including AI-assisted snippets, has been validated and executed end-to-end in the standard Kaggle environment.\n\n**Key Configurations for Reproduction**:\n\n- Global Random Seed: Set to `42` to ensure consistent results for validation splits and model initialization.\n- Environment: Standard Kaggle Python 3 Docker image.\n- R-Backend Integration: This notebook automatically installs the necessary R binaries (r-cran-midr, etc.) required for midlearn. No manual system configuration is needed.\n\n**Library Dependencies:**\n\nWhile standard libraries (`pandas`, `sklearn`, `lightgbm`, etc.) use the pre-installed versions, the following packages must be installed/updated as specified in the first cell:\n\n- `midlearn==0.1.2` - Installed via PyPI ([Link](https://pypi.org/project/midlearn/)) Developed by the participant to bridge R's `midr` capabilities into Python environments.\n- `plotnine>=0.15.1` - Updated from PyPI to ensure compatibility with recent matplotlib versions and correct grammar-of-graphics rendering using the layout operators (`&`, `|`, and `/`).\n- `scikit-misc` - Installed via PyPI, required for `loess` smoothing in visualization.","metadata":{}}]}
